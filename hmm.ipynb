{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ice-cream example\n",
    "\n",
    "states = ('HOT', 'COLD')\n",
    "symbols = ('1', '2', '3')\n",
    "\n",
    "start_prob = {\n",
    "    'HOT': 0.8,\n",
    "    'COLD': 0.2\n",
    "}\n",
    "\n",
    "trans_prob = {\n",
    "    'HOT': { 'HOT': 0.6, 'COLD': 0.4 },\n",
    "    'COLD': { 'HOT': 0.5, 'COLD': 0.5 },\n",
    "}\n",
    "\n",
    "emit_prob = {\n",
    "    'HOT': { '1': 0.2, '2': 0.4, '3': 0.4 },\n",
    "    'COLD': { '1': 0.5, '2': 0.4, '3': 0.1 },\n",
    "}\n",
    "\n",
    "sequence = ['3', '1', '3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequences = [\n",
    "    (['HOT', 'HOT', 'COLD', 'COLD', 'COLD'], ['3', '2', '2', '1', '1']),\n",
    "    (['COLD', 'COLD', 'COLD', 'COLD', 'HOT'], ['1', '2', '1', '1', '2']),\n",
    "    (['COLD', 'HOT', 'COLD', 'HOT', 'COLD'], ['1', '2', '2', '3', '1']),\n",
    "    (['HOT', 'HOT', 'HOT', 'HOT', 'COLD'], ['3', '2', '3', '3', '1']),\n",
    "    (['HOT', 'COLD', 'COLD', 'HOT', 'COLD'], ['2', '1', '1', '3', '1']),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def _normalize_prob(prob, item_set):\n",
    "    result = {}\n",
    "    if prob is None:\n",
    "        number = len(item_set)\n",
    "        for item in item_set:\n",
    "            result[item] = 1.0 / number\n",
    "    else:\n",
    "        prob_sum = 0.0\n",
    "        for item in item_set:\n",
    "            prob_sum += prob.get(item, 0)\n",
    "            \n",
    "        if prob_sum > 0:\n",
    "            for item in item_set:\n",
    "                result[item] = prob.get(item, 0) / prob_sum\n",
    "        else:\n",
    "            for item in item_set:\n",
    "                result[item] = 0\n",
    "                \n",
    "    return result\n",
    "\n",
    "def _normalize_prob_two_dim(prob, item_set1, item_set2):\n",
    "    result = {}\n",
    "    if prob is None:\n",
    "        for item in item_set1:\n",
    "            result[itme] = _normalize_prob(None, item_set2)\n",
    "    else:\n",
    "        for item in item_set1:\n",
    "            result[item] = _normalize_prob(prob.get(item), item_set2)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def _count(item, count):\n",
    "    if item not in count:\n",
    "        count[item] = 0\n",
    "    count[item] += 1\n",
    "    \n",
    "def _count_two_dim(item1, item2, count):\n",
    "    if item1 not in count:\n",
    "        count[item1] = {}\n",
    "    _count(item2, count[item1])\n",
    "    \n",
    "def _get_init_model(sequences):\n",
    "    symbol_count = {}\n",
    "    state_count = {}\n",
    "    state_symbol_count = {}\n",
    "    state_start_count = {}\n",
    "    state_trans_count = {}\n",
    "    \n",
    "    for state_list, symbol_list in sequences:\n",
    "        pre_state = None\n",
    "        for state, symbol in zip(state_list, symbol_list):\n",
    "            _count(state, state_count)\n",
    "            _count(symbol, symbol_count)\n",
    "            _count_two_dim(state, symbol, state_symbol_count)\n",
    "            if pre_state is None:\n",
    "                _count(state, state_start_count)\n",
    "            else:\n",
    "                _count_two_dim(pre_state, state, state_trans_count)\n",
    "            pre_state = state\n",
    "            \n",
    "    return Model(state_count.keys(), symbol_count.keys(),\n",
    "                state_start_count, state_trans_count, state_symbol_count)\n",
    "\n",
    "def train(sequences, delta=0.0001, smoothing=0):\n",
    "    \"\"\"\n",
    "    Use the given sequences to train a HMM model.\n",
    "    This method is an implementation of the `EM algorithm\n",
    "    <http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm>`_.\n",
    "    \n",
    "    The `delta` argument (which is defaults to 0.0001) specifies that the\n",
    "    learning algorithm will stop when the difference of the log-likelihood\n",
    "    between two consecutive iterations is less than delta.\n",
    "    \n",
    "    The `smoothing` argument is used to avoid zero probability,\n",
    "    see :py:meth:`~hmm.Model.learn`.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = _get_init_model(sequences)\n",
    "    length = len(sequences)\n",
    "    \n",
    "    old_likelihood = 0\n",
    "    for _, symbol_list in sequences:\n",
    "        old_likelihood += np.log(model.evaluate(symbol_list))\n",
    "        \n",
    "    old_likelihood /= length\n",
    "    \n",
    "    while True:\n",
    "        new_likelihood = 0\n",
    "        for _, symbol_list in sequences:\n",
    "            model.learn(symbol_list, smoothing)\n",
    "            new_likelihood += np.log(model.evaluate(symbol_list))\n",
    "            \n",
    "            new_likelihood /= length\n",
    "            \n",
    "        if abs(new_likelihood - old_likelihood) < delta:\n",
    "            break\n",
    "            \n",
    "        old_likelihood = new_likelihood\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Model\n",
    "    # init(self, states, symbols, start_prob, trans_prob, emit_prob)\n",
    "    # _forward(self, sequence)\n",
    "    # _backward(self, sequence)\n",
    "    # evaluate(self, sequence)\n",
    "    # decode(self, sequence)\n",
    "    # learn(self, sequence, smoothing=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, states, symbols, start_prob=None, trans_prob=None, emit_prob=None):\n",
    "        self._states = set(states)\n",
    "        self._symbols = set(symbols)\n",
    "        self._start_prob = _normalize_prob(start_prob, self._states)\n",
    "        self._trans_prob = _normalize_prob_two_dim(trans_prob, self._states, self._states)\n",
    "        self._emit_prob = _normalize_prob_two_dim(emit_prob, self._states, self._symbols)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"{name}({_states}, {_symbols}, {_start_prob}, {_trans_prob}, {_emit_prob})\" \\\n",
    "            .format(name=self.__class__.__name__, **self.__dict__)\n",
    "        \n",
    "    def _forward(self, sequence):\n",
    "        length = len(sequence)\n",
    "        if length == 0:\n",
    "            return []\n",
    "        \n",
    "        alpha = [{}]\n",
    "        for state in self._states:\n",
    "            alpha[0][state] = self._start_prob[state] * self._emit_prob[state][sequence[0]]\n",
    "            \n",
    "        for t in range(1, length):\n",
    "            alpha.append({})\n",
    "            for state_j in self._states:\n",
    "                prob_sum = 0\n",
    "                for state_i in self._states:\n",
    "                    prob_sum += alpha[t-1][state_i] * self._trans_prob[state_i][state_j] \\\n",
    "                    * self._emit_prob[state_j][sequence[t]]\n",
    "                alpha[t][state_j] = prob_sum\n",
    "        \n",
    "        return alpha\n",
    "    \n",
    "    def _backward(self, sequence):\n",
    "        length = len(sequence)\n",
    "        if length == 0:\n",
    "            return []\n",
    "        \n",
    "        beta = [{}]\n",
    "        for state in self._states:\n",
    "            beta[0][state] = 1\n",
    "            \n",
    "        for t in range(length-1, 0, -1):\n",
    "            beta.insert(0, {})\n",
    "            for state_i in self._states:\n",
    "                prob_sum = 0\n",
    "                for state_j in self._states:\n",
    "                    prob_sum += self._trans_prob[state_i][state_j] * \\\n",
    "                                self._emit_prob[state_j][sequence[t]] * beta[1][state_j]\n",
    "                beta[0][state_i] = prob_sum\n",
    "                \n",
    "        return beta\n",
    "    \n",
    "    def evaluate(self, sequence):\n",
    "        length = len(sequence)\n",
    "        if length == 0:\n",
    "            return 0\n",
    "        \n",
    "        prob_sum = 0\n",
    "        alpha = self._forward(sequence)\n",
    "        for state in alpha[length-1]:\n",
    "            prob_sum += alpha[length-1][state]\n",
    "            \n",
    "        return prob_sum\n",
    "    \n",
    "    def decode(self, sequence):\n",
    "        length = len(sequence)\n",
    "        if length == 0:\n",
    "            return []\n",
    "        \n",
    "        viterbi = {}\n",
    "        for state in self._states:\n",
    "            viterbi[state] = self._start_prob[state] * self._emit_prob[state][sequence[0]]\n",
    "        \n",
    "        # make path {\"state_j\":\"state_i\"}\n",
    "        path = []\n",
    "        for t in range(1, length):\n",
    "            viterbi_tmp = {}\n",
    "            prev_state = {}\n",
    "            for state_j in self._states:\n",
    "                max_prob = 0\n",
    "                max_state = None\n",
    "                for state_i in self._states:\n",
    "                    prob = viterbi[state_i] * self._trans_prob[state_i][state_j]\n",
    "                    if prob > max_prob:\n",
    "                        max_prob = prob\n",
    "                        max_state = state_i\n",
    "                viterbi_tmp[state_j] = max_prob * self._emit_prob[state_j][sequence[t]]\n",
    "                prev_state[state_j] = max_state\n",
    "            viterbi = viterbi_tmp\n",
    "            path.append(prev_state)\n",
    "        print(\"path: \", path)\n",
    "        print(\"the last viterbi \", viterbi)\n",
    "\n",
    "        # select the last time state\n",
    "        max_prob = 0\n",
    "        max_state = None\n",
    "        for state in self._states:\n",
    "            if viterbi[state] > max_prob:\n",
    "                max_prob = viterbi[state]\n",
    "                max_state = state\n",
    "        \n",
    "        # backtrace\n",
    "        result = [max_state]\n",
    "        for t in range(length-1, 0, -1):\n",
    "            max_state = path[t-1][max_state]\n",
    "            result.insert(0, max_state)\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def learn(self, sequence, smoothing=0):\n",
    "        length = len(sequence)\n",
    "        alpha = self._forward(sequence)\n",
    "        beta = self._backward(sequence)\n",
    "        \n",
    "        gamma = []\n",
    "        for t in range(length):\n",
    "            prob_sum = 0\n",
    "            gamma.append({})\n",
    "            for state in self._states:\n",
    "                prob = alpha[t][state] * beta[t][state]\n",
    "                gamma[t][state] = prob\n",
    "                prob_sum += prob\n",
    "                \n",
    "            if prob_sum == 0:\n",
    "                continue\n",
    "                \n",
    "            for state in self._states:\n",
    "                gamma[t][state] /= prob_sum\n",
    "                \n",
    "        xi = []\n",
    "        for t in range(length-1):\n",
    "            prob_sum = 0\n",
    "            xi.append({})\n",
    "            for state_i in self._states:\n",
    "                xi[t][state_i] = {}\n",
    "                for state_j in self._states:\n",
    "                    prob = alpha[t][state_i] * beta[t+1][state_j] \\\n",
    "                        * self._trans_prob[state_i][state_j] \\\n",
    "                        * self._emit_prob[state_j][sequence[t+1]]\n",
    "                    xi[t][state_i][state_j] = prob\n",
    "                    prob_sum += prob\n",
    "                    \n",
    "            if prob_sum ==0:\n",
    "                continue\n",
    "                    \n",
    "            for state_i in self._states:\n",
    "                for state_j in self._states:\n",
    "                    xi[t][state_i][state_j] /= prob_sum\n",
    "        \n",
    "        states_number = len(self._states)\n",
    "        symbols_number = len(self._symbols)\n",
    "        for state in self._states:\n",
    "            # update start probability\n",
    "            self._start_prob[state] = \\\n",
    "                (smoothing + gamma[0][state]) / (1 + states_number * smoothing)\n",
    "            \n",
    "            # update transition probability\n",
    "            gamma_sum = 0\n",
    "            for t in range(length-1):\n",
    "                gamma_sum += gamma[t][state]\n",
    "            \n",
    "            if gamma_sum > 0:\n",
    "                denominator = gamma_sum + states_number * smoothing\n",
    "                for state_j in self._states:\n",
    "                    xi_sum = 0\n",
    "                    for t in range(length-1):\n",
    "                        xi_sum += xi[t][state][state_j]\n",
    "                    self._trans_prob[state][state_j] = (smoothing + xi_sum) / denominator\n",
    "            else:\n",
    "                for state_j in self._states:\n",
    "                    self._trans_prob[state][state_j] = 0\n",
    "        \n",
    "            # update emission probability\n",
    "            gamma_sum += gamma[length-1][state]\n",
    "            emit_gamma_prob = {}\n",
    "            for symbol in self._symbols:\n",
    "                emit_gamma_prob[symbol] = 0\n",
    "                \n",
    "            for t in range(length):\n",
    "                emit_gamma_prob[sequence[t]] += gamma[t][state]\n",
    "                \n",
    "            if gamma_sum > 0:\n",
    "                denominator = gamma_sum + symbols_number * smoothing\n",
    "                for symbol in self._symbols:\n",
    "                    self._emit_prob[state][symbol] = \\\n",
    "                        (smoothing + emit_gamma_prob[symbol]) / denominator\n",
    "            else:\n",
    "                for symbol in self._symbols:\n",
    "                    self._emit_prob[state][symbol] = 0            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = train(sequences, delta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(states, symbols, start_prob, trans_prob, emit_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COLD': 0.2, 'HOT': 0.8}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COLD': {'COLD': 0.5, 'HOT': 0.5}, 'HOT': {'COLD': 0.4, 'HOT': 0.6}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._trans_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COLD': {'1': 0.5, '2': 0.4, '3': 0.1}, 'HOT': {'1': 0.2, '2': 0.4, '3': 0.4}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._emit_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  [{'HOT': 'HOT', 'COLD': 'HOT'}, {'HOT': 'COLD', 'COLD': 'COLD'}]\n",
      "the last viterbi  {'HOT': 0.012800000000000004, 'COLD': 0.003200000000000001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HOT', 'COLD', 'HOT']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.learn(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COLD': 0.06337091240109236, 'HOT': 0.9366290875989077}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._start_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COLD': {'COLD': 0.2465897166841553, 'HOT': 0.7534102833158446},\n",
       " 'HOT': {'COLD': 0.4627994955863806, 'HOT': 0.537200504413619}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._trans_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'COLD': {'1': 0.7149962695846804, '2': 0.0, '3': 0.28500373041531957},\n",
       " 'HOT': {'1': 0.18375568551007146, '2': 0.0, '3': 0.8162443144899284}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._emit_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:  [{'HOT': 'HOT', 'COLD': 'HOT'}, {'HOT': 'COLD', 'COLD': 'COLD'}]\n",
      "the last viterbi  {'HOT': 0.1555736982952833, 'COLD': 0.01777910804547891}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['HOT', 'COLD', 'HOT']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decode(sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
